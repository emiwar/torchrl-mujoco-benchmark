{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742754cc-bbe6-47e7-bd0a-126fe28aad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchrl\n",
    "import custom_torchrl_env\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8fde52-140a-44e1-b458-7354665304c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'custom_torchrl_env' from '/home/emil/Development/custom_torchrl_env/custom_torchrl_env.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(custom_torchrl_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b90f47-a0f6-4f06-a012-ee7d1a6f9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = custom_torchrl_env.RodentRunEnv(batch_size=(2048,), device='cuda', worker_thread_count=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26c9dea3-37a6-454f-a046-b56b9da32015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 14:08:08,663 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "torchrl.envs.utils.check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b6e38-a772-4ed4-afa6-d70da4b943e1",
   "metadata": {},
   "source": [
    "# Debug deadlocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41ea03-f0d5-4cb2-9049-c4ba3d6a53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.rollout(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d518b37-9953-4fca-8834-fb8a30083c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tensordict_data in tqdm.tqdm(collector):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65d272-9840-46af-ba44-245a25c26c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb570435-904f-4a79-b71c-80d1217d1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b436059d-63aa-44d6-bf47-83ed2d4919ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Passed action contains NaNs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m a \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mrand_action()\n\u001b[1;32m      3\u001b[0m a[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m----> 4\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfullphysics\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\u001b[38;5;66;03m#[\"action\"]\u001b[39;00m\n",
      "File \u001b[0;32m~/pyenvs/build_mujoco/lib/python3.10/site-packages/torchrl/envs/common.py:1409\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[1;32m   1407\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1409\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_proc_data(next_tensordict)\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/custom_torchrl_env/custom_torchrl_env.py:123\u001b[0m, in \u001b[0;36mRodentRunEnv._step\u001b[0;34m(self, statedict)\u001b[0m\n\u001b[1;32m    121\u001b[0m action \u001b[38;5;241m=\u001b[39m statedict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action\u001b[38;5;241m.\u001b[39misnan()\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed action contains NaNs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m state_size \u001b[38;5;241m=\u001b[39m mujoco\u001b[38;5;241m.\u001b[39mmj_stateSize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mj_model, mujoco\u001b[38;5;241m.\u001b[39mmjtState\u001b[38;5;241m.\u001b[39mmjSTATE_FULLPHYSICS)\n\u001b[1;32m    125\u001b[0m com_before \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulation_pool\u001b[38;5;241m.\u001b[39mgetSubtree_com())[:, \u001b[38;5;241m1\u001b[39m, :])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mValueError\u001b[0m: Passed action contains NaNs."
     ]
    }
   ],
   "source": [
    "#env.reset()\n",
    "a = env.rand_action()\n",
    "a[\"action\"] *= torch.nan\n",
    "env.step(a)[(\"next\", \"fullphysics\")]#[\"action\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a86226-bb83-4ad9-a6b7-fa965f1dd8c2",
   "metadata": {},
   "source": [
    "# From TorchRL PPO tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b41996-fe62-4a9b-8c26-caf5816f9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "import tensordict\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8fc6cdd-59d0-4d81-bfbb-a112df0e4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cells = 1024\n",
    "lr = 1e-4\n",
    "max_grad_norm = 1.0\n",
    "device = 'cuda'\n",
    "frames_per_batch = 8*1024\n",
    "total_frames = 2048*1024#256*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9556a2-12ac-4b67-aaf8-e148daac7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_batch_size = 32  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 4  # optimisation steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a52612d-1a20-4a61-8365-285d2eb1bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/pyenvs/build_mujoco/lib/python3.10/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "actor_net = torch.nn.Sequential(\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "    tensordict.nn.distributions.NormalParamExtractor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a995640-c96b-4122-af03-dfc7fbcde6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = tensordict.nn.TensorDictModule(\n",
    "    actor_net, in_keys=[\"fullphysics\"], out_keys=[\"loc\", \"scale\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b6e1ed7-fc92-423a-b9b3-a6ce210bdc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = torchrl.modules.ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=torchrl.modules.TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"min\": env.action_spec.space.low,\n",
    "        \"max\": env.action_spec.space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    # we'll need the log-prob for the numerator of the importance weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7cd5ce3-09d9-4037-b328-b6a9cd5d8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net = torch.nn.Sequential(\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = torchrl.modules.ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"fullphysics\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1f87765-7c0e-4fe8-bd8c-4b384e61ac1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running policy: torch.Size([2048])\n",
      "Running value: torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "print(\"Running policy:\", policy_module(env.reset()).shape)\n",
    "print(\"Running value:\", value_module(env.reset()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30a4de5f-d071-4002-9824-59d82fc7a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = torchrl.collectors.SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "247de1a6-95f6-4ec4-90e9-0c9d1c75a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = torchrl.data.replay_buffers.ReplayBuffer(\n",
    "    storage=torchrl.data.replay_buffers.storages.LazyTensorStorage(max_size=frames_per_batch),\n",
    "    sampler=torchrl.data.replay_buffers.samplers.SamplerWithoutReplacement(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f17d7923-181b-4974-a581-99fdfc7bf589",
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage_module = torchrl.objectives.value.GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True, device=device\n",
    ")\n",
    "\n",
    "loss_module = torchrl.objectives.ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d79182b-5d3f-4225-adf5-9060e3dfdcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cc376f6-8905-4ca4-acaf-5e7a1d6af9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c44e9576-b608-4ad7-811d-dc2cb646bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [01:22,  3.09it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, tensordict_data in tqdm.tqdm(enumerate(collector)):\n",
    "    for j in range(tensordict_data.shape[1]):\n",
    "        advantage_module(tensordict_data[:,j])\n",
    "        loss_vals = loss_module(tensordict_data[:,j])\n",
    "        loss_value = (\n",
    "            loss_vals[\"loss_objective\"]\n",
    "            + loss_vals[\"loss_critic\"]\n",
    "            + loss_vals[\"loss_entropy\"]\n",
    "        )\n",
    "        loss_value.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "        optim.step()\n",
    "        optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c2a0ea1e-7d01-4288-8b27-29038af385ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25890.765432098764"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_frames / 81#173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "db63cba5-9bc4-43da-ab98-394ddccc0209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8192/8192 [01:00<00:00, 134.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for t in tqdm.trange(total_frames // 256):\n",
    "    env.rand_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2d885889-550b-40f4-b3ce-86cb3555e690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8192/8192 [00:37<00:00, 221.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for t in tqdm.trange(total_frames // 256):\n",
    "    env.simulation_pool.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a5606d6-2e62-4aed-911a-3c981487acc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.306] Training...\n",
      "0 [6.816] Logging...\n",
      "0 [6.817] Evaluating...\n",
      "1 [7.934] Training...\n",
      "1 [14.475] Logging...\n",
      "2 [14.738] Training...\n",
      "2 [21.190] Logging...\n",
      "3 [21.436] Training...\n",
      "3 [28.078] Logging...\n",
      "4 [28.332] Training...\n",
      "4 [34.816] Logging...\n",
      "5 [35.068] Training...\n",
      "5 [42.020] Logging...\n",
      "6 [42.260] Training...\n",
      "6 [49.436] Logging...\n",
      "7 [49.679] Training...\n",
      "7 [56.774] Logging...\n",
      "8 [57.025] Training...\n",
      "8 [63.933] Logging...\n",
      "9 [64.185] Training...\n",
      "9 [71.032] Logging...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Passed action contains NaNs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m eval_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# We iterate over the collector until it reaches the total number of frames it was\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# designed to collect:\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tensordict_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(collector):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# we now have a batch of data to work with. Let's learn something from it.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39ms_time), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# We'll need an \"advantage\" signal to make PPO work.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# We re-compute it at each epoch as its value depends on the value\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# network which is updated in the inner loop.\u001b[39;00m\n",
      "File \u001b[0;32m~/pyenvs/build_mujoco/lib/python3.10/site-packages/torchrl/collectors/collectors.py:952\u001b[0m, in \u001b[0;36mSyncDataCollector.iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_frames:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 952\u001b[0m     tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tensordict_out\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m total_frames:\n",
      "File \u001b[0;32m~/pyenvs/build_mujoco/lib/python3.10/site-packages/torchrl/_utils.py:469\u001b[0m, in \u001b[0;36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _os_is_windows \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_distributed_rpc\u001b[38;5;241m.\u001b[39mPyRRef):\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_value()\n\u001b[0;32m--> 469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pyenvs/build_mujoco/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pyenvs/build_mujoco/lib/python3.10/site-packages/torchrl/collectors/collectors.py:1069\u001b[0m, in \u001b[0;36mSyncDataCollector.rollout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1068\u001b[0m     env_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle\n\u001b[0;32m-> 1069\u001b[0m env_output, env_next_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_and_maybe_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_output:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;66;03m# ad-hoc update shuttle\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m     next_data \u001b[38;5;241m=\u001b[39m env_output\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/pyenvs/build_mujoco/lib/python3.10/site-packages/torchrl/envs/common.py:2576\u001b[0m, in \u001b[0;36mEnvBase.step_and_maybe_reset\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m   2534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_and_maybe_reset\u001b[39m(\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;28mself\u001b[39m, tensordict: TensorDictBase\n\u001b[1;32m   2536\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TensorDictBase, TensorDictBase]:\n\u001b[1;32m   2537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a step in the environment and (partially) resets it if needed.\u001b[39;00m\n\u001b[1;32m   2538\u001b[0m \n\u001b[1;32m   2539\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;124;03m            is_shared=False)\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2576\u001b[0m     tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;66;03m# done and truncated are in done_keys\u001b[39;00m\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;66;03m# We read if any key is done.\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m     tensordict_ \u001b[38;5;241m=\u001b[39m step_mdp(\n\u001b[1;32m   2580\u001b[0m         tensordict,\n\u001b[1;32m   2581\u001b[0m         keep_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2586\u001b[0m         done_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_keys,\n\u001b[1;32m   2587\u001b[0m     )\n",
      "File \u001b[0;32m~/pyenvs/build_mujoco/lib/python3.10/site-packages/torchrl/envs/common.py:1409\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[1;32m   1407\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1409\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_proc_data(next_tensordict)\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/custom_torchrl_env/custom_torchrl_env.py:125\u001b[0m, in \u001b[0;36mRodentRunEnv._step\u001b[0;34m(self, statedict)\u001b[0m\n\u001b[1;32m    123\u001b[0m action \u001b[38;5;241m=\u001b[39m statedict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action\u001b[38;5;241m.\u001b[39misnan()\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed action contains NaNs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m state_size \u001b[38;5;241m=\u001b[39m mujoco\u001b[38;5;241m.\u001b[39mmj_stateSize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mj_model, mujoco\u001b[38;5;241m.\u001b[39mmjtState\u001b[38;5;241m.\u001b[39mmjSTATE_FULLPHYSICS)\n\u001b[1;32m    127\u001b[0m com_before \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulation_pool\u001b[38;5;241m.\u001b[39mgetSubtree_com())[:, \u001b[38;5;241m1\u001b[39m, :])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mValueError\u001b[0m: Passed action contains NaNs."
     ]
    }
   ],
   "source": [
    "s_time = time.time()\n",
    "logs = collections.defaultdict(list)\n",
    "#pbar = tqdm.tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    print(i, \"[{:.3f}]\".format(time.time()-s_time), \"Training...\")\n",
    "    for j in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for k in range(frames_per_batch // sub_batch_size):\n",
    "            #print(i, j, k, \"Drawing sample\")\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optimization step\n",
    "            #print(i, j, k, \"Backward pass\")\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            #print(i, j, k, \"Optimization step\")\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            #print(i, j, k, \"Sub-batch done\")\n",
    "    print(i, \"[{:.3f}]\".format(time.time()-s_time), \"Logging...\")\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    #pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        print(i, \"[{:.3f}]\".format(time.time()-s_time), \"Evaluating...\")\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our ``env`` horizon).\n",
    "        # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with torchrl.envs.utils.set_exploration_type(torchrl.envs.utils.ExplorationType.MEAN), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(100, policy_module)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    #pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5972113c-8547-45ce-9aba-a48701e222ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'reward': [-1.5837199687957764,\n",
       "              -1.9704928398132324,\n",
       "              -1.7500295639038086,\n",
       "              -0.8590556383132935,\n",
       "              -0.6554573774337769,\n",
       "              -0.7430774569511414,\n",
       "              -0.4696408808231354,\n",
       "              -1.022096872329712,\n",
       "              -1.2704322338104248,\n",
       "              -1.4466164112091064],\n",
       "             'step_count': [32, 132, 164, 196, 228, 260, 292, 324, 356, 388],\n",
       "             'lr': [0.0001,\n",
       "              9.975923633360985e-05,\n",
       "              9.903926402016151e-05,\n",
       "              9.784701678661043e-05,\n",
       "              9.619397662556433e-05,\n",
       "              9.409606321741774e-05,\n",
       "              9.157348061512726e-05,\n",
       "              8.865052266813684e-05,\n",
       "              8.535533905932737e-05,\n",
       "              8.171966420818226e-05],\n",
       "             'eval reward': [-1.0644084215164185],\n",
       "             'eval reward (sum)': [-27248.85546875],\n",
       "             'eval step_count': [100]})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c0626-a60c-4cd2-b99a-e9967075fc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
